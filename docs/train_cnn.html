<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Model Training Script Documentation</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
        h1, h2 { color: #2c3e50; }
        pre { background-color: #f8f8f8; border: 1px solid #ddd; padding: 10px; }
        code { background-color: #f8f8f8; padding: 2px 6px; font-size: 1.1em; }
        .function-section { margin-top: 30px; }
        .code-block { background-color: #f8f8f8; padding: 15px; border-radius: 5px; margin-bottom: 30px; }
    </style>
</head>
<body>

    <h1>Model Training Script Documentation</h1>

    <p>This document provides an overview of the code for training Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) models on the e-SNLI dataset. The script trains, validates, and saves model checkpoints for use in subsequent tests or predictions.</p>

    <h2>Overview</h2>
    <p>The script involves the following steps:</p>
    <ul>
        <li>Data loading and preprocessing</li>
        <li>Model initialization (CNN or LSTM)</li>
        <li>Model training with optimization and early stopping</li>
        <li>Model evaluation on validation data</li>
        <li>Saving the best performing model based on validation F1 score</li>
    </ul>

    <h2>Functions</h2>

    <div class="function-section">
        <h3>train_model</h3>
        <p>This function trains the model using the specified training and validation data loaders. It iterates through the training data for the given number of epochs, updates the model using the Adam optimizer, and applies early stopping based on validation performance.</p>
        <h4>Parameters:</h4>
        <ul>
            <li><code>model</code> (torch.nn.Module): The model to be trained (e.g., CNN).</li>
            <li><code>train_dl</code> (BatchSampler): DataLoader for the training dataset.</li>
            <li><code>dev_dl</code> (BatchSampler): DataLoader for the validation dataset.</li>
            <li><code>optimizer</code> (torch.optim.Optimizer): The optimizer used for training (e.g., AdamW).</li>
            <li><code>scheduler</code> (torch.optim.lr_scheduler.LambdaLR): Learning rate scheduler.</li>
            <li><code>n_epochs</code> (int): Number of training epochs.</li>
            <li><code>early_stopping</code> (EarlyStopping): Early stopping handler to stop training if the model performance does not improve.</li>
        </ul>
        <h4>Returns:</h4>
        <ul>
            <li><code>best_model_weights</code> (dict): The model state dictionary with the best weights based on validation performance.</li>
            <li><code>best_val</code> (dict): A dictionary containing the best validation performance metrics.</li>
        </ul>
        <h4>Code:</h4>
        <pre class="code-block">
def train_model(model: torch.nn.Module,
                train_dl: BatchSampler, dev_dl: BatchSampler,
                optimizer: torch.optim.Optimizer,
                scheduler: torch.optim.lr_scheduler.LambdaLR,
                n_epochs: int,
                early_stopping: EarlyStopping) -> (Dict, Dict):
    loss_f = torch.nn.CrossEntropyLoss()

    best_val, best_model_weights = {'val_f1': 0}, None  # Initialize best performance tracking

    for ep in range(n_epochs):
        model.train()  # Set the model to training mode
        for batch in tqdm(train_dl, desc='Training'):
            optimizer.zero_grad()  # Clear previous gradients
            logits = model(batch[0])  # Forward pass
            loss = loss_f(logits, batch[1])  # Calculate the loss
            loss.backward()  # Backpropagate the loss
            optimizer.step()  # Update model parameters

        val_p, val_r, val_f1, val_loss, _, _ = eval_model(model, dev_dl)  # Evaluate model
        current_val = {
            'val_p': val_p, 'val_r': val_r, 'val_f1': val_f1,
            'val_loss': val_loss, 'ep': ep
        }

        if current_val['val_f1'] > best_val['val_f1']:
            best_val = current_val
            best_model_weights = model.state_dict()

        scheduler.step(val_loss)
        if early_stopping.step(val_f1):
            print('Early stopping...')
            break

    return best_model_weights, best_val
        </pre>
    </div>

    <div class="function-section">
        <h3>eval_model</h3>
        <p>This function evaluates the model on a test or validation dataset, calculating the classification performance based on metrics such as accuracy or F1 score. It returns the confusion matrix and the evaluation metrics.</p>
        <h4>Parameters:</h4>
        <ul>
            <li><code>model</code> (torch.nn.Module): The model to be evaluated (e.g., CNN).</li>
            <li><code>test_dl</code> (BucketBatchSampler): DataLoader for the test/validation dataset.</li>
            <li><code>measure</code> (str, optional): The metric to use for evaluation, either 'acc' for accuracy or 'f1' for F1 score. Default is None.</li>
        </ul>
        <h4>Returns:</h4>
        <ul>
            <li><code>p</code> (float): Precision score (if F1 score is used).</li>
            <li><code>r</code> (float): Recall score (if F1 score is used).</li>
            <li><code>f1</code> (float): F1 score.</li>
            <li><code>loss</code> (float): Average loss during evaluation.</li>
            <li><code>labels_all</code> (list): List of true labels for the dataset.</li>
            <li><code>prediction</code> (list): List of predicted labels.</li>
        </ul>
        <h4>Code:</h4>
        <pre class="code-block">
def eval_model(model: torch.nn.Module, test_dl: BucketBatchSampler,
               measure=None):
    model.eval()  # Set the model to evaluation mode

    loss_f = torch.nn.CrossEntropyLoss()  # Cross-entropy loss function

    with torch.no_grad():  # Disable gradient calculation during evaluation
        labels_all = []
        logits_all = []
        losses = []
        for batch in tqdm(test_dl, desc="Evaluation"):  # Loop through batches
            logits_val = model(batch[0])  # Forward pass
            loss_val = loss_f(logits_val, batch[1])  # Calculate loss
            losses.append(loss_val.item())

            labels_all += batch[1].detach().cpu().numpy().tolist()  # Collect true labels
            logits_all += logits_val.detach().cpu().numpy().tolist()  # Collect model predictions

        prediction = np.argmax(np.array(logits_all), axis=-1)  # Get predicted class labels

        if measure == 'acc':
            f1 = accuracy_score(labels_all, prediction)  # Compute accuracy
        else:
            p, r, f1, _ = precision_recall_fscore_support(labels_all,
                                                          prediction,
                                                          average='macro')  # Compute F1 score

        print(confusion_matrix(labels_all, prediction))  # Print confusion matrix

    return p, r, f1, np.mean(losses), labels_all, prediction
        </pre>
    </div>

    <h2>Main Function</h2>
    <p>The main function of the script initializes the model, sets up the data loaders, and manages the training loop. It also saves the model checkpoints after training is complete. Additionally, it handles model evaluation in 'test' mode if specified.</p>

    <h4>Code:</h4>
    <pre class="code-block">
if __name__ == "__main__":
    for i in range(1, 6):
        args = {
            "gpu": False,
            "init_only": False,
            "seed": 73,
            "labels": 3,
            "dataset_dir": "data/e-SNLI/dataset",
            "model_path": [f"data/models/snli/cnn/cnn_{i}", f"data/models/snli/random_cnn/cnn_{i}"],
            "batch_size": 256,
            "lr": 0.0001,
            "epochs": 100,
            "mode": "test",
            "patience": 5,
            "model": "cnn",
            "embedding_dir": "./glove/",
            "dropout": 0.05,
            "embedding_dim": 300,
            "in_channels": 1,
            "out_channels": 300,
            "kernel_heights": [4, 5, 6, 7],
            "stride": 1,
            "padding": 0
        }
        for index, path in enumerate(args["model_path"]):
            if index == 1:
                args["init_only"] = True

            random.seed(args["seed"])
            np.random.seed(args["seed"])
            torch.manual_seed(args["seed"])

            device = torch.device("cuda") if args["gpu"] else torch.device("cpu")

            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

            collate_fn = partial(collate_nli, tokenizer=tokenizer, device=device,
                                 return_attention_masks=False, pad_to_max_length=False)

            sort_key = lambda x: len(x[0]) + len(x[1])

            model = CNN_MODEL(tokenizer, args, n_labels=3).to(device)

            train = NLIDataset(args["dataset_dir"], type='train', salient_features=True)
            dev = NLIDataset(args["dataset_dir"], type='dev')

            train_dl = BucketBatchSampler(batch_size=args["batch_size"],
                                          sort_key=sort_key, dataset=train,
                                          collate_fn=collate_fn)
            dev_dl = BucketBatchSampler(batch_size=args["batch_size"],
                                        sort_key=sort_key, dataset=dev,
                                        collate_fn=collate_fn)

            optimizer = AdamW(model.parameters(), lr=args["lr"])
            scheduler = ReduceLROnPlateau(optimizer)
            es = EarlyStopping(patience=args["patience"], percentage=False, mode='max', min_delta=0.0)

            if not args["init_only"]:
                best_model_w, best_perf = train_model(model, train_dl, dev_dl,
                                                    optimizer, scheduler,
                                                    args["epochs"],
                                                    es)
            else:
                best_model_w, best_perf = model.state_dict(), {'val_f1': 0}

            checkpoint = {
                'performance': best_perf,
                'args': args,
                'model': best_model_w,
            }

            torch.save(checkpoint, path)
        </pre>
    </body>
</html>
